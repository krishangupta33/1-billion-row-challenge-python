{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Billion Row Challenge\n",
    "\n",
    "ğŸ’ª The challenge\n",
    "\n",
    "Your mission, should you choose to accept it, is to write a program that retrieves temperature measurement values from a text file and calculates the min, mean, and max temperature per weather station. There's just one caveat: the file has 1,000,000,000 rows! That's more than 10 GB of data! ğŸ˜±\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-  Pure Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 1000000000it [08:31, 1953846.14it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[1;32m     35\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 36\u001b[0m city_stats \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_calculate_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/measurements.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHelsinki\u001b[39m\u001b[38;5;124m'\u001b[39m,city_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHelsinki\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36mread_and_calculate_stats\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(file, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing data\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m         city, temp \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m         temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(temp)\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m city_stats:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_and_calculate_stats(file_name):\n",
    "    city_stats = {}\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in tqdm(file, desc=\"Processing data\"):\n",
    "            city, temp = line.strip().split(';')\n",
    "            temp = float(temp)\n",
    "\n",
    "            if city in city_stats:\n",
    "                stats = city_stats[city]\n",
    "                stats['count'] += 1\n",
    "                stats['total'] += temp\n",
    "                if temp < stats['min']:\n",
    "                    stats['min'] = temp\n",
    "                if temp > stats['max']:\n",
    "                    stats['max'] = temp\n",
    "            else:\n",
    "                city_stats[city] = {\n",
    "                    'min': temp,\n",
    "                    'max': temp,\n",
    "                    'total': temp,\n",
    "                    'count': 1\n",
    "                }\n",
    "\n",
    "    # Calculate mean from total and count\n",
    "    for city, stats in city_stats.items():\n",
    "        stats['mean'] = stats['total'] / stats['count']\n",
    "        del stats['total'], stats['count']  # Optional: Remove these if no longer needed\n",
    "\n",
    "    return city_stats\n",
    "\n",
    "# Main execution\n",
    "start_time = time.time()\n",
    "city_stats = read_and_calculate_stats('data/measurements.txt')\n",
    "end_time = time.time()\n",
    "\n",
    "print('Helsinki',city_stats['Helsinki'])\n",
    "print('Guatemala City,',city_stats['Guatemala City'],'\\n')\n",
    "\n",
    "print(f\"Time elapsed: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 1001it [02:35,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 155.37 seconds\n",
      "                          temperature_min  temperature_max  temperature_mean\n",
      "city                                                                        \n",
      "Abha                                -34.0             68.6         17.998179\n",
      "Abidjan                             -26.2             76.6         25.999310\n",
      "AbÃ©chÃ©                              -21.0             81.0         29.409223\n",
      "Accra                               -23.4             77.6         26.406375\n",
      "Addis Ababa                         -34.8             66.2         16.010328\n",
      "...                                   ...              ...               ...\n",
      "Zanzibar City                       -25.3             78.3         26.003238\n",
      "ZÃ¼rich                              -42.4             59.6          9.296035\n",
      "station_name,measurement              NaN              NaN               NaN\n",
      "ÃœrÃ¼mqi                              -49.9             54.5          7.398977\n",
      "Ä°zmir                               -30.4             66.3         17.894226\n",
      "\n",
      "[414 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_data(file_path, chunk_size=1000000):\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Initialize an empty DataFrame to accumulate results\n",
    "    accumulated_results = pd.DataFrame()\n",
    "\n",
    "    # Initialize reader object\n",
    "    reader = pd.read_csv(file_path, sep=';', header=None, names=['city', 'temp'], chunksize=chunk_size)\n",
    "\n",
    "    # Process each chunk\n",
    "    for chunk in tqdm(reader, desc=\"Processing chunks\"):\n",
    "        # Group by 'city' and calculate min, max, and mean for the chunk\n",
    "        results = chunk.groupby('city')['temp'].agg(['min', 'max', 'mean']).rename(columns={\n",
    "            'min': 'temperature_min',\n",
    "            'max': 'temperature_max',\n",
    "            'mean': 'temperature_mean'\n",
    "        })\n",
    "        # Append chunk results to the accumulated results\n",
    "        accumulated_results = pd.concat([accumulated_results, results])\n",
    "\n",
    "    # Final aggregation to ensure city stats are correct across all chunks\n",
    "    final_results = accumulated_results.groupby(accumulated_results.index).agg({\n",
    "        'temperature_min': 'min',\n",
    "        'temperature_max': 'max',\n",
    "        'temperature_mean': 'mean'\n",
    "    })\n",
    "\n",
    "    end_time = time.time()  # End timing\n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "\n",
    "    print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")  # Print the elapsed time\n",
    "    return final_results\n",
    "\n",
    "# Specify your file path\n",
    "file_path = 'data/measurements.txt'\n",
    "city_stats = process_data(file_path)\n",
    "print(city_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 0.04 seconds\n",
      "shape: (413, 4)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ city       â”† temperature_min â”† temperature_max â”† temperature_mean â”‚\n",
      "â”‚ ---        â”† ---             â”† ---             â”† ---              â”‚\n",
      "â”‚ str        â”† f64             â”† f64             â”† f64              â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ Medan      â”† -8.7            â”† 63.0            â”† 26.352709        â”‚\n",
      "â”‚ Belgrade   â”† -21.5           â”† 42.2            â”† 12.470008        â”‚\n",
      "â”‚ Mombasa    â”† -7.3            â”† 59.7            â”† 26.455896        â”‚\n",
      "â”‚ Almaty     â”† -24.9           â”† 40.7            â”† 10.087885        â”‚\n",
      "â”‚ Bujumbura  â”† -10.5           â”† 61.0            â”† 23.834524        â”‚\n",
      "â”‚ â€¦          â”† â€¦               â”† â€¦               â”† â€¦                â”‚\n",
      "â”‚ Moscow     â”† -34.7           â”† 39.9            â”† 5.914118         â”‚\n",
      "â”‚ Alexandria â”† -11.2           â”† 55.2            â”† 20.053718        â”‚\n",
      "â”‚ Hanga Roa  â”† -15.3           â”† 54.4            â”† 20.582998        â”‚\n",
      "â”‚ San Diego  â”† -14.6           â”† 61.5            â”† 17.888807        â”‚\n",
      "â”‚ Kankan     â”† -9.6            â”† 60.1            â”† 26.22214         â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_data_polars(file_path, chunk_size=1000000):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a lazy scan of the CSV file\n",
    "    df = pl.scan_csv(file_path, separator=\";\", has_header=False, new_columns=[\"city\", \"temp\"])\n",
    "\n",
    "    # Define aggregations and group by 'city'\n",
    "    results = (\n",
    "        df.lazy()\n",
    "        .group_by(\"city\")\n",
    "        .agg(\n",
    "            [\n",
    "                pl.col(\"temp\").min().alias(\"temperature_min\"),\n",
    "                pl.col(\"temp\").max().alias(\"temperature_max\"),\n",
    "                pl.col(\"temp\").mean().alias(\"temperature_mean\"),\n",
    "            ]\n",
    "        )\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "    return results\n",
    "\n",
    "# Specify your file path\n",
    "file_path = \"data/1million.txt\"\n",
    "city_stats = process_data_polars(file_path)\n",
    "print(city_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The kernel crashed when we didn't enable streaming. However, for 1 million rows, Polars took only 0.04 seconds. \n",
    "\n",
    "Now calculating 1 billion rows using Polars with streaming enabled.\n",
    "\n",
    "\n",
    "**Streaming Aggregation**: The `collect(streaming=True)` option instructs Polars to perform the aggregation in a streaming fashion. Instead of building the entire result in memory, Polars processes the data in smaller chunks and yields results as they become available. This approach is more memory-efficient and allows for processing larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 27.80 seconds\n",
      "shape: (414, 4)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ city          â”† temperature_min â”† temperature_max â”† temperature_mean â”‚\n",
      "â”‚ ---           â”† ---             â”† ---             â”† ---              â”‚\n",
      "â”‚ str           â”† f64             â”† f64             â”† f64              â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ Perth         â”† -32.9           â”† 64.6            â”† 18.696792        â”‚\n",
      "â”‚ Skopje        â”† -35.2           â”† 61.3            â”† 12.395606        â”‚\n",
      "â”‚ Odesa         â”† -38.5           â”† 60.4            â”† 10.701866        â”‚\n",
      "â”‚ Zanzibar City â”† -25.3           â”† 78.3            â”† 26.003186        â”‚\n",
      "â”‚ Chicago       â”† -41.2           â”† 58.8            â”† 9.791128         â”‚\n",
      "â”‚ â€¦             â”† â€¦               â”† â€¦               â”† â€¦                â”‚\n",
      "â”‚ Minneapolis   â”† -39.7           â”† 56.9            â”† 7.80093          â”‚\n",
      "â”‚ Ghanzi        â”† -27.3           â”† 71.6            â”† 21.396405        â”‚\n",
      "â”‚ Chittagong    â”† -25.7           â”† 77.9            â”† 25.912396        â”‚\n",
      "â”‚ Singapore     â”† -22.8           â”† 75.9            â”† 27.01894         â”‚\n",
      "â”‚ Kabul         â”† -38.2           â”† 61.8            â”† 12.099089        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "\n",
    "\n",
    "def process_data_polars_streaming(file_path):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a lazy scan of the CSV file\n",
    "    df = pl.scan_csv(file_path, separator=\";\", has_header=False, new_columns=[\"city\", \"temp\"])\n",
    "\n",
    "    # Define aggregations and group by 'city' with streaming enabled\n",
    "    results = (\n",
    "        df.lazy()\n",
    "        .group_by(\"city\")\n",
    "        .agg(\n",
    "            [\n",
    "                pl.col(\"temp\").min().alias(\"temperature_min\"),\n",
    "                pl.col(\"temp\").max().alias(\"temperature_max\"),\n",
    "                pl.col(\"temp\").mean().alias(\"temperature_mean\"),\n",
    "            ]\n",
    "        )\n",
    "        .collect(streaming=True)  # Enable streaming\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "    return results\n",
    "\n",
    "# Specify your file path\n",
    "file_path = \"data/measurements.txt\"\n",
    "city_stats = process_data_polars_streaming(file_path)\n",
    "print(city_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With streaming = True, Polars took 27 s to compute 1 billion rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- DuckDB Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         city  min_temp  max_temp   avg_temp\n",
      "0                        Abha     -13.5      55.0  17.685526\n",
      "1                     Abidjan      -1.7      57.5  25.419055\n",
      "2                      AbÃ©chÃ©      -6.6      61.4  29.634817\n",
      "3                       Accra      -4.3      54.7  26.078281\n",
      "4                 Addis Ababa     -24.8      46.1  15.702610\n",
      "..                        ...       ...       ...        ...\n",
      "409             Zanzibar City      -4.4      54.4  25.501149\n",
      "410                    ZÃ¼rich     -20.5      32.3   9.370703\n",
      "411  station_name,measurement       NaN       NaN        NaN\n",
      "412                    ÃœrÃ¼mqi     -20.5      37.3   7.140071\n",
      "413                     Ä°zmir      -8.9      48.1  18.216854\n",
      "\n",
      "[1577661 rows x 4 columns]\n",
      "\n",
      "Time elapsed: 74.60 seconds\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Start time tracking\n",
    "start_time = time.time()\n",
    "\n",
    "parquet_file = pq.ParquetFile('data/measurements.parquet')\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process in chunks\n",
    "for i in range(parquet_file.num_row_groups):\n",
    "    table = parquet_file.read_row_group(i)\n",
    "    df = table.to_pandas()\n",
    "    city_stats = df.groupby('city')['temp'].agg(['min', 'max', 'mean']).reset_index()\n",
    "    results.append(city_stats)\n",
    "\n",
    "final_results = pd.concat(results)\n",
    "final_results.columns = ['city', 'min_temp', 'max_temp', 'avg_temp']\n",
    "\n",
    "# End time tracking and calculate elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the results and elapsed time\n",
    "print(final_results)\n",
    "print(f\"\\nTime elapsed: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Implementation\n",
    "\n",
    "Dask is a flexible library for parallel computing in Python. It is a great tool for parallelizing data processing tasks, such as reading large files, filtering, and aggregating data. Dask provides a high-level API that is similar to Pandas, making it easy to use for data scientists and analysts familiar with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
